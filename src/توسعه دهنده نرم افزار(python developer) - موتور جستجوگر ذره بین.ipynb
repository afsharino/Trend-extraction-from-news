{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67ddee59",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <h2 align=\"center\">Trend Extraction from News</h2>\n",
    "    <h3 align=\"center\">Zarebin Search Engine - Entrance Project for Internship</h3>\n",
    "    <h4 align=\"center\"><a href=\"https://t.me/afsharino\">Mohammad Afshari</a></h4>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6acde6a",
   "metadata": {},
   "source": [
    "<style>\n",
    ".aligncenter {\n",
    "    text-align: center;\n",
    "}\n",
    "</style>\n",
    "<p class=\"aligncenter\">\n",
    "    <img src = \"../images/zarebin.png\"  height=400 width= 750>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ee706f",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4da8c239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scientific \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Parse html\n",
    "import base64\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Concurrecy\n",
    "import multiprocessing as mp\n",
    "import asyncio\n",
    "\n",
    "# Pre and postproccessing \n",
    "from hazm import Normalizer, WordTokenizer, Lemmatizer\n",
    "from hazm.utils import stopwords_list\n",
    "import re \n",
    "\n",
    "# data and time\n",
    "import khayyam\n",
    "\n",
    "# Others\n",
    "import os\n",
    "import glob\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae431eb",
   "metadata": {},
   "source": [
    "# Look for Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e69d04e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/afsharino/Desktop/Trend-extraction-from-news/src\n",
      "files in dataset directory are: ['asriran_all.csv', 'mehrnews_all.csv', 'khabaronline_all.csv', 'yjcnews_all.csv', 'tasnimnews_all.csv', 'farsnews_all.csv', 'varzesh3_all.csv', 'isna_all.csv', 'entekhab_all.csv', 'anapress_all.csv', 'shana_all.csv', 'jahannews_all.csv', 'mashreghnews_all.csv', '.~lock.entekhab_all.csv#', 'iscanews_all.csv']\n"
     ]
    }
   ],
   "source": [
    "print(f'Current working directory: {os.getcwd()}')\n",
    "path = '/home/afsharino/Desktop/Trend-extraction-from-news/datasets'\n",
    "print(f'files in dataset directory are: {os.listdir(path)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effb12a7",
   "metadata": {},
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63e95db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataReader:\n",
    "    def __init__(self, data=None):\n",
    "        print('DataReader called...')\n",
    "        self.data = data\n",
    "    \n",
    "    # This method reads all csv files in given path\n",
    "    def read_dataset(self, path:str) -> None:\n",
    "        print(f'Reading datasets...')\n",
    "        \n",
    "        try:\n",
    "            data = dict()\n",
    "            for file in tqdm(glob.glob(path)):\n",
    "                # Get csv file's name\n",
    "                file_name = file.split('/')[-1].split('_')[0]\n",
    "        \n",
    "                # Read csv file and save in dictionary\n",
    "                data[file_name] = pd.read_csv(file,nrows=20)\n",
    "                print(f'data with name {file_name} saved successfully :)')\n",
    "            \n",
    "            self.data = data\n",
    "            print(f'Process of reading datasets completed successfully :)')\n",
    "        except Exception as e:\n",
    "            print(f'Unfortunately Process of reading datasets failed :(')\n",
    "            print(f'Error Raised:{e}')\n",
    "    \n",
    "    # This method decodes the html source to browser format\n",
    "    def decode_html_file(self, html_source:str) -> str:\n",
    "        try:\n",
    "            # Decode html source to utf-8\n",
    "            html_doc = base64.b64decode(html_source).decode('utf-8')\n",
    "            return html_doc\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f'Error raised: Can not decode the file because: {e}')      \n",
    "            \n",
    "    # This method Parses the html doc to soup object\n",
    "    def parse_html_doc(self, html_doc:str) -> BeautifulSoup:\n",
    "        try:\n",
    "            # Parse html to soup object\n",
    "            soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "            return soup\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f'Error raised: Can not parse the html doc because: {e}')\n",
    "    \n",
    "    # This method extracts title from the news\n",
    "    def get_title(self, series:pd.core.series.Series) -> str:\n",
    "        # Unpack series\n",
    "        html_source, url = series[0], series[1]\n",
    "        \n",
    "        # Decode html source\n",
    "        html_doc = self.decode_html_file(html_source)\n",
    "        \n",
    "        # Parse html to soup\n",
    "        soup = self.parse_html_doc(html_doc)    \n",
    "\n",
    "        try:\n",
    "            # Get title of the news\n",
    "            return soup.title.text\n",
    "\n",
    "        except Exception as e:\n",
    "            if soup == None:\n",
    "                print('Soup is None!')\n",
    "                print(f'URL= {url}')\n",
    "                print('--------------------\\n')\n",
    "                return np.nan\n",
    "\n",
    "            elif soup.title == None:\n",
    "                print('Soup.title is None!')\n",
    "                print(f'URL= {url}')\n",
    "                print(soup.text)\n",
    "\n",
    "                print('--------------------\\n')\n",
    "                return np.nan\n",
    "            else:\n",
    "                print(f'Error: {e}')\n",
    "    \n",
    "    # This method adds extracted titles to corrosponding row in dataframes\n",
    "    def add_title_to_df(self, data):\n",
    "        data['title'] =  data[['html', 'url']].apply(lambda x: self.get_title(x),axis=1)\n",
    "        return data\n",
    "    \n",
    "    # This method does the proccess of add title for all dataframes\n",
    "    def add_all_titles(self) -> None:\n",
    "        print(f'Start adding titles to dataframes...')\n",
    "        \n",
    "        for source in tqdm(self.data.keys()):\n",
    "            print(f'{source} in progress...\\n')\n",
    "            \n",
    "            self.data[source] = self.parallelize(self.data[source])\n",
    "            \n",
    "            print(f'{source} titles added successfully :)')\n",
    "            print(f'---------------------------------------------------------------------\\n\\n')\n",
    "            \n",
    "    # This method extracts dates from the news\n",
    "    def get_date(self, series:pd.core.series.Series, source:str) -> str:\n",
    "        # Unpack series\n",
    "        html_source, url = series[0], series[1]\n",
    "\n",
    "        # Decode html source\n",
    "        html_doc = self.decode_html_file(html_source)\n",
    "        \n",
    "        # Parse html to soup\n",
    "        soup = self.parse_html_doc(html_doc) \n",
    "        \n",
    "        #------------------ asriiran ------------------\n",
    "        if source == 'asriran':\n",
    "            try:\n",
    "                # Get date out of the source\n",
    "                date = soup.find_all('div', {'class' : 'news_nav header_pdate'})[0].\\\n",
    "                text.split(\"                    \")[2].strip(\" \")\n",
    "                return date\n",
    "\n",
    "            except Exception as e:\n",
    "                if soup == None:\n",
    "                    print('Soup is None!')\n",
    "                    print(f'URL= {url}')\n",
    "                    print('--------------------\\n')\n",
    "                    return np.nan\n",
    "\n",
    "                elif not soup.find_all('div', {'class' : 'news_nav header_pdate'}):\n",
    "                    print('soup.find_all is empty!')\n",
    "                    print(f'URL= {url}')\n",
    "                    print(soup.text)\n",
    "                    print('--------------------\\n')\n",
    "                    return np.nan\n",
    "                else:\n",
    "                    print(f'Error: {e}')\n",
    "                    \n",
    "        #------------------ mehrnews ------------------\n",
    "        if source == 'mehrnews':\n",
    "            try:\n",
    "                # Get date out of the source\n",
    "                date = soup.find('div', {'class': 'col-6 col-sm-4 item-date'}).\\\n",
    "                text.split('،')[0].strip('\\n')\n",
    "                return date\n",
    "\n",
    "            except Exception as e:\n",
    "                if soup == None:\n",
    "                    print('Soup is None!')\n",
    "                    print(f'URL= {url}')\n",
    "                    print('--------------------\\n')\n",
    "                    return np.nan\n",
    "\n",
    "                elif not soup.find('div', {'class': 'col-6 col-sm-4 item-date'}):\n",
    "                    print('soup.find is empty!')\n",
    "                    print(f'URL= {url}')\n",
    "                    print(soup.text)\n",
    "                    print('--------------------\\n')\n",
    "                    return np.nan\n",
    "                else:\n",
    "                    print(f'Error: {e}')\n",
    "                    \n",
    "        #------------------ khabaronline ------------------\n",
    "        if source == 'khabaronline':\n",
    "             try:\n",
    "                # Get date out of the source\n",
    "                date = soup.find('div', {'class': 'col-6 col-sm-4 item-date'}).\\\n",
    "                text.split('-')[0].strip('\\n')\n",
    "                return date\n",
    "\n",
    "             except Exception as e:\n",
    "                if soup == None:\n",
    "                    print('Soup is None!')\n",
    "                    print(f'URL= {url}')\n",
    "                    print('--------------------\\n')\n",
    "                    return np.nan\n",
    "\n",
    "                elif not soup.find('div', {'class': 'col-6 col-sm-4 item-date'}):\n",
    "                    print('soup.find is empty!')\n",
    "                    print(f'URL= {url}')\n",
    "                    print(soup.text)\n",
    "                    print('--------------------\\n')\n",
    "                    return np.nan\n",
    "                else:\n",
    "                    print(f'Error: {e}')\n",
    "                    \n",
    "        #------------------ yjcnews ------------------\n",
    "        if source == 'yjcnews':\n",
    "            try:\n",
    "                # Get date out of the source\n",
    "                date = soup.find_all('span', {'class': 'date-color-news'})[1].text.strip()\n",
    "                return date\n",
    "        \n",
    "            except Exception as e:\n",
    "                if soup == None:\n",
    "                    print('Soup is None!')\n",
    "                    print(f'URL= {url}')\n",
    "                    print('--------------------\\n')\n",
    "                    return np.nan\n",
    "\n",
    "                elif not soup.find_all('span', {'class': 'date-color-news'}):\n",
    "                    print('soup.find_all is empty!')\n",
    "                    print(f'URL= {url}')\n",
    "                    print(soup.text)\n",
    "                    print('--------------------\\n')\n",
    "                    return np.nan\n",
    "                else:\n",
    "                    print(f'Error: {e}')\n",
    "                    \n",
    "        #------------------ anapress ------------------\n",
    "        if source == 'anapress':\n",
    "            try:\n",
    "                # Get date out of the source\n",
    "                date = soup.find('div', {'class': 'news-date'}).text.split('-')[0].strip()\n",
    "                return date\n",
    "        \n",
    "            except Exception as e:\n",
    "                if soup == None:\n",
    "                    print('Soup is None!')\n",
    "                    print(f'URL= {url}')\n",
    "                    print('--------------------\\n')\n",
    "                    return np.nan\n",
    "        \n",
    "                elif not soup.find('div', {'class': 'news-date'}):\n",
    "                    print('soup.find is empty!')\n",
    "                    print(f'URL= {url}')\n",
    "                    print(soup.text)\n",
    "                    print('--------------------\\n')\n",
    "                    return np.nan\n",
    "                else:\n",
    "                    print(f'Error: {e}')\n",
    "                    \n",
    "        #------------------ tasnimnews ------------------\n",
    "        if source == 'tasnimnews':\n",
    "            try:\n",
    "                # Get date out of the source\n",
    "                date = soup.find('li', {'class': 'time'}).text.split('-')[0].strip()\n",
    "                return date\n",
    "        \n",
    "            except Exception as e:\n",
    "                if soup == None:\n",
    "                    print('Soup is None!')\n",
    "                    print(f'URL= {url}')\n",
    "                    print('--------------------\\n')\n",
    "                    return np.nan\n",
    "\n",
    "                elif not soup.find('li', {'class': 'time'}):\n",
    "                    if not soup.find('time'):\n",
    "                        print('soup.find is empty!')\n",
    "                        print(f'URL= {url}')\n",
    "                        print(soup.text)\n",
    "                        print('--------------------\\n')\n",
    "                        return np.nan\n",
    "                    else:\n",
    "                        date1 = soup.find('time').text.split('-')[0].strip()\n",
    "                        return date1\n",
    "                else:\n",
    "                    print(f'Error: {e}')\n",
    "                    \n",
    "        #------------------ farsnews ------------------\n",
    "        if source == 'farsnews':\n",
    "            try:\n",
    "                # Get date out of the source\n",
    "                date = soup.find('time').text.split('\\n')[2].strip(' \\r')\n",
    "                return date\n",
    "        \n",
    "            except Exception as e:\n",
    "                if soup == None:\n",
    "                    print('Soup is None!')\n",
    "                    print(f'URL= {url}')\n",
    "                    print('--------------------\\n')\n",
    "                    return np.nan\n",
    "\n",
    "                elif not soup.find('time'):\n",
    "                    if not soup.find('div', {'class': 'data-box d-flex justify-content-start align-items-center'}):\n",
    "                        print('soup.find is empty!')\n",
    "                        print(f'URL= {url}')\n",
    "                        print(soup.text)\n",
    "                        print('--------------------\\n')\n",
    "                        return np.nan\n",
    "                    else:\n",
    "                        date1 = soup.find('div', {'class': 'data-box d-flex justify-content-start align-items-center'}).\\\n",
    "                        find_all('span')[2].text.strip(' \\r\\n')\n",
    "                        return date1\n",
    "\n",
    "                else:\n",
    "                    print(f'Error: {e}')\n",
    "    \n",
    "        #------------------ varzesh3 ------------------\n",
    "        if source == 'varzesh3':\n",
    "            try:\n",
    "                # Get date out of the source\n",
    "                date = soup.find('div', {'class': 'news-info'}).find_all('span')[1].text.split('ساعت')[0].strip()\n",
    "                return date\n",
    "\n",
    "            except Exception as e:\n",
    "                if soup == None:\n",
    "                    print('Soup is None!')\n",
    "                    print(f'URL= {url}')\n",
    "                    print('--------------------\\n')\n",
    "                    return np.nan\n",
    "\n",
    "                elif not soup.find('div', {'class': 'news-info'}):\n",
    "                    if not soup.find('span', {'class': 'date'}):\n",
    "                        print('soup.find is empty!')\n",
    "                        print(f'URL= {url}')\n",
    "                        print(soup.text)\n",
    "                        print('--------------------\\n')\n",
    "                        return np.nan\n",
    "                    else:\n",
    "                        date1 = soup.find('span', {'class': 'date'}).text.split('ساعت')[0].strip(' -')\n",
    "                        return date1\n",
    "\n",
    "                else:\n",
    "                    print(f'Error: {e}')\n",
    "\n",
    "        #------------------ isna ------------------\n",
    "        if source == 'isna':\n",
    "            try:\n",
    "                # Get date out of the source\n",
    "                date = soup.find('span', {'class': 'text-meta'}).text.split('/')[0].strip()\n",
    "                return date\n",
    "\n",
    "            except Exception as e:\n",
    "                if soup == None:\n",
    "                    print('Soup is None!')\n",
    "                    print(f'URL= {url}')\n",
    "                    print('--------------------\\n')\n",
    "                    return np.nan\n",
    "\n",
    "                elif not soup.find('span', {'class': 'text-meta'}):\n",
    "                        print('soup.find is empty!')\n",
    "                        print(f'URL= {url}')\n",
    "                        print(soup.text)\n",
    "                        print('--------------------\\n')\n",
    "                        return np.nan\n",
    "\n",
    "                else:\n",
    "                    print(f'Error: {e}')\n",
    "\n",
    "        #------------------ entekhab ------------------\n",
    "        if source == 'entekhab':\n",
    "                try:\n",
    "                    # Get date out of the source\n",
    "                    date = soup.find('div', {'class': 'news_nav news_pdate_c col-xs-36 col-sm-18'}).\\\n",
    "                    text.split('-')[1].strip()\n",
    "                    return date\n",
    "\n",
    "                except Exception as e:\n",
    "                    if soup == None:\n",
    "                        print('Soup is None!')\n",
    "                        print(f'URL= {url}')\n",
    "                        print('--------------------\\n')\n",
    "                        return np.nan\n",
    "\n",
    "                    elif not soup.find('div', {'class': 'news_nav news_pdate_c col-xs-36 col-sm-18'}):\n",
    "                            print('soup.find is empty!')\n",
    "                            print(f'URL= {url}')\n",
    "                            print(soup.text)\n",
    "                            print('--------------------\\n')\n",
    "                            return np.nan\n",
    "\n",
    "                    else:\n",
    "                        print(f'Error: {e}')\n",
    "\n",
    "        #------------------ shana ------------------\n",
    "        if source == 'shana':\n",
    "            try:\n",
    "                # Get date out of the source\n",
    "                date = soup.find('div', {'class': 'col-6 col-sm-4'}).text.split('-')[0].strip(' \\n')\n",
    "                return date\n",
    "\n",
    "            except Exception as e:\n",
    "                if soup == None:\n",
    "                    print('Soup is None!')\n",
    "                    print(f'URL= {url}')\n",
    "                    print('--------------------\\n')\n",
    "                    return np.nan\n",
    "\n",
    "                elif not soup.find('div', {'class': 'col-6 col-sm-4'}):\n",
    "                    #if not soup.find('span', {'class': 'date'}):\n",
    "                        print('soup.find is empty!')\n",
    "                        print(f'URL= {url}')\n",
    "                        print(soup.text)\n",
    "                        print('--------------------\\n')\n",
    "                        return np.nan\n",
    "                    #else:\n",
    "                        #date1 = soup.find('span', {'class': 'date'}).text.split('ساعت')[0].strip(' -')\n",
    "                        #return date1\n",
    "\n",
    "                else:\n",
    "                    print(f'Error: {e}')\n",
    "\n",
    "        #------------------ jahannews ------------------\n",
    "        if source == 'jahannews':\n",
    "            try:\n",
    "                # Get date out of the source\n",
    "                date = ' '.join(soup.find('div', {'id': 'docDiv3Date'}).text.split(' ')[1:4])\n",
    "                return date\n",
    "\n",
    "            except Exception as e:\n",
    "                if soup == None:\n",
    "                    print('Soup is None!')\n",
    "                    print(f'URL= {url}')\n",
    "                    print('--------------------\\n')\n",
    "                    return np.nan\n",
    "\n",
    "                elif not soup.find('div', {'id': 'docDiv3Date'}):\n",
    "                        print('soup.find is empty!')\n",
    "                        print(f'URL= {url}')\n",
    "                        print(soup.text)\n",
    "                        print('--------------------\\n')\n",
    "                        return np.nan\n",
    "\n",
    "                else:\n",
    "                    print(f'Error: {e}')\n",
    "\n",
    "        #------------------ mashreghnews ------------------\n",
    "        if source == 'mashreghnews':\n",
    "            try:\n",
    "                # Get date out of the source\n",
    "                date = soup.find('div', {'class':'col-xs-4 head-date'}).text.split('-')[0].strip(' \\n')\n",
    "                return date\n",
    "\n",
    "            except Exception as e:\n",
    "                if soup == None:\n",
    "                    print('Soup is None!')\n",
    "                    print(f'URL= {url}')\n",
    "                    print('--------------------\\n')\n",
    "                    return np.nan\n",
    "\n",
    "                elif not soup.find('div', {'class':'col-xs-4 head-date'}):\n",
    "                    #if not soup.find('span', {'class': 'date'}):\n",
    "                        print('soup.find is empty!')\n",
    "                        print(f'URL= {url}')\n",
    "                        print(soup.text)\n",
    "                        print('--------------------\\n')\n",
    "                        return np.nan\n",
    "                    #else:\n",
    "                        #date1 = soup.find('span', {'class': 'date'}).text.split('ساعت')[0].strip(' -')\n",
    "                        #return date1\n",
    "\n",
    "                else:\n",
    "                    print(f'Error: {e}')\n",
    "\n",
    "        #------------------ iscanews ------------------\n",
    "        if source == 'iscanews':\n",
    "            try:\n",
    "                # Get date out of the source\n",
    "                date = soup.find('div', {'class':'col-6 col-sm-4 item-date'})\\\n",
    "                .text.split('-')[0].strip(' \\n')\n",
    "                return date\n",
    "\n",
    "            except Exception as e:\n",
    "                if soup == None:\n",
    "                    print('Soup is None!')\n",
    "                    print(f'URL= {url}')\n",
    "                    print('--------------------\\n')\n",
    "                    return np.nan\n",
    "\n",
    "                elif not soup.find('div', {'class':'col-6 col-sm-4 item-date'}):\n",
    "                    #if not soup.find('span', {'class': 'date'}):\n",
    "                        print('soup.find is empty!')\n",
    "                        print(f'URL= {url}')\n",
    "                        print(soup.text)\n",
    "                        print('--------------------\\n')\n",
    "                        return np.nan\n",
    "                    #else:\n",
    "                        #date1 = soup.find('span', {'class': 'date'}).text.split('ساعت')[0].strip(' -')\n",
    "                        #return date1\n",
    "\n",
    "                else:\n",
    "                    print(f'Error: {e}')\n",
    "        else:\n",
    "            print('None of above')\n",
    "    \n",
    "    # This method adds extracted dates to corrosponding row in dataframes\n",
    "    def add_date_to_df(self, data, source):\n",
    "        print(f'source is : {source}')\n",
    "        data['date'] =  data[['html', 'url']].apply(lambda x: self.get_date(x, source),axis=1)\n",
    "        return data\n",
    "    \n",
    "    # This method does the proccess of add date for all dataframes\n",
    "    def add_all_dates(self) -> None:\n",
    "        print(f'Start adding dates to dataframes...')\n",
    "        \n",
    "        for source in tqdm(self.data.keys()):\n",
    "            print(f'{source} in progress...\\n')\n",
    "        \n",
    "            self.data[source] = self.parallelize(self.data[source], source)\n",
    "\n",
    "            \n",
    "            print(f'{source} dates added successfully :)')\n",
    "            print(f'---------------------------------------------------------------------\\n\\n')\n",
    "    \n",
    "    # This method parallelizes the process of adding title and date to dataframe\n",
    "    def parallelize(self, data, source=None, n_cores=8):\n",
    "        # Split series to n segment\n",
    "        splitted_data = np.array_split(data, n_cores)\n",
    "        \n",
    "        pool = mp.Pool((n_cores))\n",
    "        \n",
    "        if source == None:\n",
    "            data = pd.concat(pool.map(self.add_title_to_df, splitted_data))\n",
    "        else:\n",
    "            data = pd.concat(pool.starmap(self.add_date_to_df, list(zip(splitted_data,3* [source]))))\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ddfd72",
   "metadata": {},
   "source": [
    "# Preproccess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ce730fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preproccessor:\n",
    "    def __init__(self, data):\n",
    "        print(f'Preproccesor called...')\n",
    "        self.data = data\n",
    "    \n",
    "    # This method drops NaNs from dataframes\n",
    "    def drop_nan_values(self) -> None:\n",
    "        print(f'Start dropping NaN values...')\n",
    "        \n",
    "        try:\n",
    "            for source in tqdm(self.data.keys()):\n",
    "                print(f'{source} in progress...\\n')\n",
    "                print(f'Number of rows before dropping NaNs: {self.data[source].shape[0]}')\n",
    "\n",
    "                self.data[source] = self.data[source].dropna()\n",
    "                \n",
    "                print(f'Number of rows after dropping NaNs: {self.data[source].shape[0]}')\n",
    "                print(f'{source} NaNs dropped successfully :)')\n",
    "                print(f'---------------------------------------------------------------------\\n\\n')\n",
    "                \n",
    "            print(f'Process of dropping NaNs completed successfully :)')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f'Unfortunately Process of dropping NaNs failed :(')\n",
    "            print(f'Error Raised:{e}')\n",
    "    \n",
    "    # This method rmoves duplicate rows from dataframes\n",
    "    def remove_duplicate_rows(self) -> None:\n",
    "        print(f'Start removing duplicate rows...')\n",
    "        \n",
    "        try:\n",
    "            for source in tqdm(self.data.keys()):\n",
    "                print(f'{source} in progress...\\n')\n",
    "                print(f'Number of rows before removing duplicates: {self.data[source].shape[0]}')\n",
    "\n",
    "                self.data[source] = self.data[source].drop_duplicates()\n",
    "                \n",
    "                print(f'Number of rows after removing duplicates: {self.data[source].shape[0]}')\n",
    "                print(f'{source} Duplicate rows removed successfully :)')\n",
    "                print(f'---------------------------------------------------------------------\\n\\n')\n",
    "                \n",
    "            print(f'Process of removing duplicates completed successfully :)')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f'Unfortunately Process of  removing duplicates failed :(')\n",
    "            print(f'Error Raised:{e}')\n",
    "            \n",
    "    # This method converts dataes to same format\n",
    "    def convert_date(self):\n",
    "        pass\n",
    "    \n",
    "    # This method merges dataframes with each other\n",
    "    def merge_data_frames(self) -> None:\n",
    "        print(f'Start appending dataframes...')\n",
    "        \n",
    "        data_to_merge = []\n",
    "        \n",
    "        try:\n",
    "            for source in tqdm(self.data.keys()):\n",
    "                print(f'{source} in progress...\\n')\n",
    "\n",
    "                data_to_merge.append(self.data[source])\n",
    "                \n",
    "                print(f'{source} appended successfully :)')\n",
    "                print(f'---------------------------------------------------------------------\\n\\n')\n",
    "                \n",
    "            print(f'Process of appending completed successfully :)')\n",
    "            \n",
    "            print(f'Start merging dataframes...')\n",
    "            \n",
    "            self.data = pd.concat(data_to_merge)\n",
    "            print(f'Process of merging completed successfully :)')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f'Unfortunately Process of  removing duplicates failed :(')\n",
    "            print(f'Error Raised:{e}')\n",
    "            \n",
    "    # This method normalizes the given text\n",
    "    def text_normalizer(self, text):\n",
    "        normalizer = Normalizer()\n",
    "        normalized_text = normalizer.normalize(text)\n",
    "        return normalized_text\n",
    "    \n",
    "    # This method normalizes the given column\n",
    "    def normalizer(self, data):\n",
    "        data = data.apply(lambda x: self.text_normalizer(x))\n",
    "        return data\n",
    "    \n",
    "    # main normalizer\n",
    "    def normalize(self):\n",
    "        print(f'Start normalizing...')\n",
    "        self.data.title = self.parallelize(self.data.title, self.normalizer)\n",
    "        print(f'Normalizing finished')\n",
    "        \n",
    "    # This method Tokenizes the given text                     \n",
    "    def word_Tokenizer(self, text):\n",
    "        tokenizer = WordTokenizer()\n",
    "        tokenized_text = tokenizer.tokenize(text)\n",
    "        return tokenized_text\n",
    "    \n",
    "    # This method tokenizes the given column\n",
    "    def tokenizer(self, data):\n",
    "        data = data.apply(lambda x: self.word_Tokenizer(x))\n",
    "        return data\n",
    "    \n",
    "    # main tokenizer\n",
    "    def tokenize(self):\n",
    "        print(f'Start tokenizing...')\n",
    "        self.data.title = self.parallelize(self.data.title, self.tokenizer)\n",
    "        print(f'tokenizing finished')\n",
    "        \n",
    "    # This method lemmatize the given text                     \n",
    "    def token_lemmatizer(self, text):\n",
    "        lemmatized_text = []\n",
    "        for token in text:\n",
    "            lemmatizer = Lemmatizer()\n",
    "            lemmatized_text.append(lemmatizer.lemmatize(token))\n",
    "            \n",
    "        return lemmatized_text\n",
    "    \n",
    "    # This method lemmatize the given column\n",
    "    def lemmatizer(self, data):\n",
    "        data = data.apply(lambda x: self.token_lemmatizer(x))\n",
    "        return data\n",
    "    \n",
    "    # main lemmatizer\n",
    "    def lemmatize(self):\n",
    "        print(f'Start lemmatizing...')\n",
    "        self.data.title = self.parallelize(self.data.title, self.lemmatizer)\n",
    "        print(f'lemmatizing finished')\n",
    "    \n",
    "    def remove_stopwords(self):\n",
    "        for row in self.data.title:\n",
    "            for token in row:\n",
    "                if token in stopwords_list():\n",
    "                    row.remove(token)\n",
    "        print('stopwords removed!')\n",
    "        \n",
    "    def parallelize(self, data, func, n_cores=8):\n",
    "        # Split series to n segment\n",
    "        splitted_data = np.array_split(data, n_cores)\n",
    "                          \n",
    "        pool = mp.Pool((n_cores))\n",
    "        data = pd.concat(pool.map(func, splitted_data))\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec115895",
   "metadata": {},
   "source": [
    "# Postproccessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5910694",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Postproccessor:\n",
    "    def __init__(self, data):\n",
    "        print('Postproccessor is called...')\n",
    "        self.data = data\n",
    "    \n",
    "    def remove_single_chars(self):\n",
    "        for row in self.data.title:\n",
    "            for token in row:\n",
    "                if len(token)==1:\n",
    "                    row.remove(token)\n",
    "        print('single words removed!')\n",
    "\n",
    "\n",
    "    def remove_punctuations(self):\n",
    "        regex_pattern = re.compile(pattern=r'[^\\w\\s()]')\n",
    "        for row in self.data.title:\n",
    "                for token in row:\n",
    "                    token = re.sub(pattern=regex_pattern, repl=r'', string=token)\n",
    "\n",
    "        print('punctuations removed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2ba559",
   "metadata": {},
   "source": [
    "# Trend Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aff58146",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrenExtractor:\n",
    "    def __init__(self, data):\n",
    "        print('trend extractor is called...')\n",
    "        self.data =data\n",
    "        self.overall_trend = ''\n",
    "        self.daily_trend = ''\n",
    "        \n",
    "    def find_overall_trend(self):\n",
    "        all_words = []\n",
    "        for row in self.data.title:\n",
    "            for token in row:\n",
    "                all_words.append(token)\n",
    "                \n",
    "        trend = Counter(all_words).most_common(1)\n",
    "        print(f'The overall trend is {trend[0][0]} with {trend[0][1]} occurence\\n')\n",
    "        print(f'The top 10 trends are {Counter(all_words).most_common(10)}\\n')\n",
    "\n",
    "    def find_daily_trend(self):\n",
    "        data.groupby(data['date'])\n",
    "        \"\"\"Unfortunately due to a lack of time  to parse dates to correct and same format\n",
    "        I couldn't complete this part but if dates format were correct, the idea is to\n",
    "        group by date and in each group count the most frequent word and then return the\n",
    "        frequent word in each day\"\"\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2a5b6c6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataReader called...\n",
      "Reading datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faa35d7c5dd54a8a82a82a01b0709867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data with name asriran saved successfully :)\n",
      "data with name mehrnews saved successfully :)\n",
      "data with name khabaronline saved successfully :)\n",
      "data with name yjcnews saved successfully :)\n",
      "data with name tasnimnews saved successfully :)\n",
      "data with name farsnews saved successfully :)\n",
      "data with name varzesh3 saved successfully :)\n",
      "data with name isna saved successfully :)\n",
      "data with name entekhab saved successfully :)\n",
      "data with name anapress saved successfully :)\n",
      "data with name shana saved successfully :)\n",
      "data with name jahannews saved successfully :)\n",
      "data with name mashreghnews saved successfully :)\n",
      "data with name iscanews saved successfully :)\n",
      "Process of reading datasets completed successfully :)\n",
      "Start adding titles to dataframes...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a1129a26b894e26bdbe975a2c017a3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asriran in progress...\n",
      "\n",
      "Soup.title is None!\n",
      "URL= https://www.asriran.com/fa/news/823655\n",
      "403 Forbidden\n",
      "Request forbidden by administrative rules.\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      "Soup.title is None!\n",
      "URL= https://www.asriran.com/fa/news/823655\n",
      "403 Forbidden\n",
      "Request forbidden by administrative rules.\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      "Soup.title is None!\n",
      "URL= https://www.asriran.com/fa/news/725405/%D8%A7%D9%86%D8%AA%D8%AE%D8%A7%D8%A8%D8%A7%D8%AA-%D8%AE%D8%A7%D9%86%D9%87-%D8%B3%DB%8C%D9%86%D9%85%D8%A7-%DA%86%D9%87-%D8%B2%D9%85%D8%A7%D9%86%DB%8C-%D8%A8%D8%B1%DA%AF%D8%B2%D8%A7%D8%B1-%D9%85%DB%8C%E2%80%8C%D8%B4%D9%88%D8%AF\n",
      "403 Forbidden\n",
      "Request forbidden by administrative rules.\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      "Soup.title is None!\n",
      "URL= https://www.asriran.com/fa/news/725405/%D8%A7%D9%86%D8%AA%D8%AE%D8%A7%D8%A8%D8%A7%D8%AA-%D8%AE%D8%A7%D9%86%D9%87-%D8%B3%DB%8C%D9%86%D9%85%D8%A7-%DA%86%D9%87-%D8%B2%D9%85%D8%A7%D9%86%DB%8C-%D8%A8%D8%B1%DA%AF%D8%B2%D8%A7%D8%B1-%D9%85%DB%8C%E2%80%8C%D8%B4%D9%88%D8%AF\n",
      "403 Forbidden\n",
      "Request forbidden by administrative rules.\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      "Soup.title is None!\n",
      "URL= https://www.asriran.com/fa/news/467978/%D8%AF%D9%88%D8%B1%D8%A8%DB%8C%D9%86-%D8%B9%DB%8C%D9%86%DA%A9%DB%8C\n",
      "403 Forbidden\n",
      "Request forbidden by administrative rules.\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      "Soup.title is None!\n",
      "URL= https://www.asriran.com/fa/news/470597\n",
      "403 Forbidden\n",
      "Request forbidden by administrative rules.\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      "Error raised: Can not decode the file because: 'utf-8' codec can't decode byte 0xd8 in position 2718: invalid continuation byte\n",
      "Error raised: Can not parse the html doc because: object of type 'NoneType' has no len()\n",
      "Soup is None!\n",
      "URL= https://www.asriran.com/fa/news/794824/%D8%AC%D9%87%D8%A7%D9%86%DA%AF%DB%8C%D8%B1%DB%8C-%D8%A2%D9%86%DA%86%D9%87-%D8%AF%D8%B1-%D8%AA%D9%88%D8%A7%D9%86-%D8%AF%D9%88%D9%84%D8%AA-%D9%88-%DA%A9%D8%B4%D9%88%D8%B1-%D8%A8%D8%A7%D8%B4%D8%AF-%D8%A8%D8%B1%D8%A7%DB%8C-%D8%AE%D9%88%D8%B2%D8%B3%D8%AA%D8%A7%D9%86-%D8%A7%D9%86%D8%AC%D8%A7%D9%85-%D9%85%DB%8C%E2%80%8C%D8%AF%D9%87%DB%8C%D9%85\n",
      "--------------------\n",
      "\n",
      "Error raised: Can not decode the file because: 'utf-8' codec can't decode byte 0xd8 in position 2603: invalid continuation byte\n",
      "Error raised: Can not parse the html doc because: object of type 'NoneType' has no len()\n",
      "Soup is None!\n",
      "URL= https://www.asriran.com/fa/news/734777/%D8%A7%D9%81%D8%B2%D8%A7%DB%8C%D8%B4-2-%D9%85%DB%8C%D9%84%DB%8C%D9%88%D9%86%DB%8C-%D9%85%D8%AA%D9%88%D8%B3%D8%B7-%D9%82%DB%8C%D9%85%D8%AA-%D9%85%D8%B3%DA%A9%D9%86-%D8%AF%D8%B1-%D8%AA%D9%87%D8%B1%D8%A7%D9%86\n",
      "--------------------\n",
      "\n",
      "Error raised: Can not decode the file because: 'utf-8' codec can't decode byte 0xd8 in position 2603: invalid continuation byte\n",
      "Error raised: Can not parse the html doc because: object of type 'NoneType' has no len()\n",
      "Soup is None!\n",
      "URL= https://www.asriran.com/fa/news/734777/%D8%A7%D9%81%D8%B2%D8%A7%DB%8C%D8%B4-2-%D9%85%DB%8C%D9%84%DB%8C%D9%88%D9%86%DB%8C-%D9%85%D8%AA%D9%88%D8%B3%D8%B7-%D9%82%DB%8C%D9%85%D8%AA-%D9%85%D8%B3%DA%A9%D9%86-%D8%AF%D8%B1-%D8%AA%D9%87%D8%B1%D8%A7%D9%86\n",
      "--------------------\n",
      "\n",
      "Error raised: Can not decode the file because: 'utf-8' codec can't decode byte 0xd8 in position 2718: invalid continuation byte\n",
      "Error raised: Can not parse the html doc because: object of type 'NoneType' has no len()\n",
      "Soup is None!\n",
      "URL= https://www.asriran.com/fa/news/794824/%D8%AC%D9%87%D8%A7%D9%86%DA%AF%DB%8C%D8%B1%DB%8C-%D8%A2%D9%86%DA%86%D9%87-%D8%AF%D8%B1-%D8%AA%D9%88%D8%A7%D9%86-%D8%AF%D9%88%D9%84%D8%AA-%D9%88-%DA%A9%D8%B4%D9%88%D8%B1-%D8%A8%D8%A7%D8%B4%D8%AF-%D8%A8%D8%B1%D8%A7%DB%8C-%D8%AE%D9%88%D8%B2%D8%B3%D8%AA%D8%A7%D9%86-%D8%A7%D9%86%D8%AC%D8%A7%D9%85-%D9%85%DB%8C%E2%80%8C%D8%AF%D9%87%DB%8C%D9%85\n",
      "--------------------\n",
      "\n",
      "Soup.title is None!\n",
      "URL= https://www.asriran.com/fa/news/470597\n",
      "403 Forbidden\n",
      "Request forbidden by administrative rules.\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      "Soup.title is None!\n",
      "URL= https://www.asriran.com/fa/news/830897/%D9%85%D8%AF%DB%8C%D8%B1%D8%B9%D8%A7%D9%85%D9%84-%D8%A7%DB%8C%D8%B1%D8%A7%D9%86-%D8%AE%D9%88%D8%AF%D8%B1%D9%88-%D8%B9%D8%B1%D8%B6%D9%87-%D9%87%D9%81%D8%AA-%D9%85%D8%AD%D8%B5%D9%88%D9%84-%D8%AC%D8%AF%DB%8C%D8%AF-%D8%AA%D9%88%D8%B3%D8%B7-%D8%A7%DB%8C%D8%B1%D8%A7%D9%86-%D8%AE%D9%88%D8%AF%D8%B1%D9%88-%D8%AF%D8%B1-%D8%B3%D8%A7%D9%84-%D8%A2%DB%8C%D9%86%D8%AF%D9%87-%D8%A7%D8%B3%D8%A7%D9%85%DB%8C-%D9%85%D8%AD%D8%B5%D9%88%D9%84%D8%A7%D8%AA-%D8%AC%D8%AF%DB%8C%D8%AF\n",
      "403 Forbidden\n",
      "Request forbidden by administrative rules.\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      "asriran titles added successfully :)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "mehrnews in progress...\n",
      "\n",
      "mehrnews titles added successfully :)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "khabaronline in progress...\n",
      "\n",
      "khabaronline titles added successfully :)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "yjcnews in progress...\n",
      "\n",
      "yjcnews titles added successfully :)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "tasnimnews in progress...\n",
      "\n",
      "Error raised: Can not decode the file because: 'utf-8' codec can't decode byte 0xc2 in position 5331: invalid continuation byte\n",
      "Error raised: Can not parse the html doc because: object of type 'NoneType' has no len()\n",
      "Soup is None!\n",
      "URL= https://www.tasnimnews.com/fa/news/1399/08/08/2378826/%D8%B1%D9%88%D8%A7%DB%8C%D8%AA-%D8%B1%D9%88%D8%B2%D9%87%D8%A7%DB%8C-%D8%A2%D8%BA%D8%A7%D8%B2%DB%8C%D9%86-%D8%AC%D9%86%DA%AF-%D8%A7%D8%B2-%D8%B2%D8%A8%D8%A7%D9%86-%D8%AE%D9%84%D8%A8%D8%A7%D9%86%D8%A7%D9%86-%D9%86%DB%8C%D8%B1%D9%88%DB%8C-%D9%87%D9%88%D8%A7%DB%8C%DB%8C-%D8%A7%D8%B1%D8%AA%D8%B4\n",
      "--------------------\n",
      "\n",
      "Error raised: Can not decode the file because: 'utf-8' codec can't decode byte 0xc2 in position 5331: invalid continuation byte\n",
      "Error raised: Can not parse the html doc because: object of type 'NoneType' has no len()\n",
      "Soup is None!\n",
      "URL= https://www.tasnimnews.com/fa/news/1399/08/08/2378826/%D8%B1%D9%88%D8%A7%DB%8C%D8%AA-%D8%B1%D9%88%D8%B2%D9%87%D8%A7%DB%8C-%D8%A2%D8%BA%D8%A7%D8%B2%DB%8C%D9%86-%D8%AC%D9%86%DA%AF-%D8%A7%D8%B2-%D8%B2%D8%A8%D8%A7%D9%86-%D8%AE%D9%84%D8%A8%D8%A7%D9%86%D8%A7%D9%86-%D9%86%DB%8C%D8%B1%D9%88%DB%8C-%D9%87%D9%88%D8%A7%DB%8C%DB%8C-%D8%A7%D8%B1%D8%AA%D8%B4\n",
      "--------------------\n",
      "\n",
      "tasnimnews titles added successfully :)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "farsnews in progress...\n",
      "\n",
      "farsnews titles added successfully :)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "varzesh3 in progress...\n",
      "\n",
      "varzesh3 titles added successfully :)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "isna in progress...\n",
      "\n",
      "isna titles added successfully :)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "entekhab in progress...\n",
      "\n",
      "entekhab titles added successfully :)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "anapress in progress...\n",
      "\n",
      "Soup.title is None!\n",
      "URL= https://ana.press/fa/news/639600/%D8%A7%D8%B3%D8%AA%D9%81%D8%A7%D8%AF%D9%87-%D8%A7%D8%B2-%D8%B8%D8%B1%D9%81%DB%8C%D8%AA%E2%80%8C-%D9%87%D9%86%D8%B1%DB%8C-%D8%AF%D8%A7%D9%86%D8%B4%DA%AF%D8%A7%D9%87-%D8%A2%D8%B2%D8%A7%D8%AF-%D8%A7%D8%B3%D9%84%D8%A7%D9%85%DB%8C-%D8%A8%D8%B1%D8%A7%DB%8C-%D8%AA%D9%88%D8%B3%D8%B9%D9%87-%D9%85%D8%B9%D8%A7%D8%B1%D9%81-%D8%AF%DB%8C%D9%86%DB%8C\n",
      "403 Forbidden\n",
      "Request forbidden by administrative rules.\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      "Soup.title is None!\n",
      "URL= https://ana.press/fa/news/639600/%D8%A7%D8%B3%D8%AA%D9%81%D8%A7%D8%AF%D9%87-%D8%A7%D8%B2-%D8%B8%D8%B1%D9%81%DB%8C%D8%AA%E2%80%8C-%D9%87%D9%86%D8%B1%DB%8C-%D8%AF%D8%A7%D9%86%D8%B4%DA%AF%D8%A7%D9%87-%D8%A2%D8%B2%D8%A7%D8%AF-%D8%A7%D8%B3%D9%84%D8%A7%D9%85%DB%8C-%D8%A8%D8%B1%D8%A7%DB%8C-%D8%AA%D9%88%D8%B3%D8%B9%D9%87-%D9%85%D8%B9%D8%A7%D8%B1%D9%81-%D8%AF%DB%8C%D9%86%DB%8C\n",
      "403 Forbidden\n",
      "Request forbidden by administrative rules.\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      "Soup.title is None!\n",
      "URL= https://ana.press/fa/news/813459/%D8%B1%DB%8C%DB%8C%D8%B3-%D8%B3%D8%A7%D8%B2%D9%85%D8%A7%D9%86-%D9%85%D8%AD%DB%8C%D8%B7-%D8%B2%DB%8C%D8%B3%D8%AA-%D9%87%D9%81%D8%AA%D9%87-%D8%A2%DB%8C%D9%86%D8%AF%D9%87-%D8%A8%D9%87-%D9%85%D8%B5%D8%B1-%D9%85%DB%8C%E2%80%8C%D8%B1%D9%88%D8%AF\n",
      "403 Forbidden\n",
      "Request forbidden by administrative rules.\n",
      "\n",
      "\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Soup.title is None!\n",
      "URL= https://ana.press/fa/news/501207/%DB%B6-%D9%86%DA%A9%D8%AA%D9%87-%D8%AF%D8%B1-%D9%85%D9%88%D8%B1%D8%AF-%D9%88%D8%B1%D8%B2%D8%B4-%D8%AF%D8%B1-%D9%87%D9%88%D8%A7%DB%8C-%DA%AF%D8%B1%D9%85\n",
      "403 Forbidden\n",
      "Request forbidden by administrative rules.\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      "Soup.title is None!\n",
      "URL= https://ana.press/fa/news/501207/%DB%B6-%D9%86%DA%A9%D8%AA%D9%87-%D8%AF%D8%B1-%D9%85%D9%88%D8%B1%D8%AF-%D9%88%D8%B1%D8%B2%D8%B4-%D8%AF%D8%B1-%D9%87%D9%88%D8%A7%DB%8C-%DA%AF%D8%B1%D9%85\n",
      "403 Forbidden\n",
      "Request forbidden by administrative rules.\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      "Soup.title is None!\n",
      "URL= https://ana.press/fa/news/659538/%D8%AF%D8%B3%D8%AA%D9%88%D8%B1-%D8%B1%D8%A6%DB%8C%D8%B3%DB%8C-%D8%A8%D8%B1%D8%A7%DB%8C-%DA%A9%D9%86%D8%A7%D8%B1-%DA%AF%D8%B0%D8%A7%D8%B4%D8%AA%D9%86-%D9%85%D8%A7%D9%86%D8%B9%E2%80%8C%D8%AA%D8%B1%D8%A7%D8%B4%D8%A7%D9%86-%D8%AA%D8%AD%D9%82%D9%82-%D8%AC%D9%87%D8%B4-%D8%B3%D8%A7%D8%AE%D8%AA-%D9%85%D8%B3%DA%A9%D9%86\n",
      "403 Forbidden\n",
      "Request forbidden by administrative rules.\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      "Soup.title is None!\n",
      "URL= https://ana.press/fa/news/659538/%D8%AF%D8%B3%D8%AA%D9%88%D8%B1-%D8%B1%D8%A6%DB%8C%D8%B3%DB%8C-%D8%A8%D8%B1%D8%A7%DB%8C-%DA%A9%D9%86%D8%A7%D8%B1-%DA%AF%D8%B0%D8%A7%D8%B4%D8%AA%D9%86-%D9%85%D8%A7%D9%86%D8%B9%E2%80%8C%D8%AA%D8%B1%D8%A7%D8%B4%D8%A7%D9%86-%D8%AA%D8%AD%D9%82%D9%82-%D8%AC%D9%87%D8%B4-%D8%B3%D8%A7%D8%AE%D8%AA-%D9%85%D8%B3%DA%A9%D9%86\n",
      "403 Forbidden\n",
      "Request forbidden by administrative rules.\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      "Soup.title is None!\n",
      "URL= https://ana.press/fa/news/663756/%D9%BE%D8%A7%D8%B3%D8%AE-%D8%A7%DB%8C%D8%B1%D8%A7%D9%86-%D8%A8%D9%87-%D8%A7%D8%AA%D9%87%D8%A7%D9%85%D8%A7%D8%AA-%D8%A7%D8%B1%D8%AF%D9%86-%D9%BE%DB%8C%D8%B1%D8%A7%D9%85%D9%88%D9%86-%D8%AA%D8%AC%D8%A7%D8%B1%D8%AA-%D9%85%D9%88%D8%A7%D8%AF-%D9%85%D8%AE%D8%AF%D8%B1\n",
      "403 Forbidden\n",
      "Request forbidden by administrative rules.\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      "Soup.title is None!\n",
      "URL= https://ana.press/fa/news/663756/%D9%BE%D8%A7%D8%B3%D8%AE-%D8%A7%DB%8C%D8%B1%D8%A7%D9%86-%D8%A8%D9%87-%D8%A7%D8%AA%D9%87%D8%A7%D9%85%D8%A7%D8%AA-%D8%A7%D8%B1%D8%AF%D9%86-%D9%BE%DB%8C%D8%B1%D8%A7%D9%85%D9%88%D9%86-%D8%AA%D8%AC%D8%A7%D8%B1%D8%AA-%D9%85%D9%88%D8%A7%D8%AF-%D9%85%D8%AE%D8%AF%D8%B1\n",
      "403 Forbidden\n",
      "Request forbidden by administrative rules.\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      "anapress titles added successfully :)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "shana in progress...\n",
      "\n",
      "shana titles added successfully :)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "jahannews in progress...\n",
      "\n",
      "jahannews titles added successfully :)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "mashreghnews in progress...\n",
      "\n",
      "mashreghnews titles added successfully :)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "iscanews in progress...\n",
      "\n",
      "iscanews titles added successfully :)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Start adding dates to dataframes...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "405467db6c844062aca0ed2558ec1d6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asriran in progress...\n",
      "\n",
      "source is : asriran\n",
      "soup.find_all is empty!\n",
      "URL= https://www.asriran.com/fa/news/823655\n",
      "403 Forbidden\n",
      "Request forbidden by administrative rules.\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      "soup.find_all is empty!\n",
      "URL= https://www.asriran.com/fa/news/823655\n",
      "403 Forbidden\n",
      "Request forbidden by administrative rules.\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      "source is : asriran\n",
      "soup.find_all is empty!\n",
      "URL= https://www.asriran.com/fa/news/725405/%D8%A7%D9%86%D8%AA%D8%AE%D8%A7%D8%A8%D8%A7%D8%AA-%D8%AE%D8%A7%D9%86%D9%87-%D8%B3%DB%8C%D9%86%D9%85%D8%A7-%DA%86%D9%87-%D8%B2%D9%85%D8%A7%D9%86%DB%8C-%D8%A8%D8%B1%DA%AF%D8%B2%D8%A7%D8%B1-%D9%85%DB%8C%E2%80%8C%D8%B4%D9%88%D8%AF\n",
      "403 Forbidden\n",
      "Request forbidden by administrative rules.\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      "soup.find_all is empty!\n",
      "URL= https://www.asriran.com/fa/news/725405/%D8%A7%D9%86%D8%AA%D8%AE%D8%A7%D8%A8%D8%A7%D8%AA-%D8%AE%D8%A7%D9%86%D9%87-%D8%B3%DB%8C%D9%86%D9%85%D8%A7-%DA%86%D9%87-%D8%B2%D9%85%D8%A7%D9%86%DB%8C-%D8%A8%D8%B1%DA%AF%D8%B2%D8%A7%D8%B1-%D9%85%DB%8C%E2%80%8C%D8%B4%D9%88%D8%AF\n",
      "403 Forbidden\n",
      "Request forbidden by administrative rules.\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      "source is : asriran\n",
      "soup.find_all is empty!\n",
      "URL= https://www.asriran.com/fa/news/467978/%D8%AF%D9%88%D8%B1%D8%A8%DB%8C%D9%86-%D8%B9%DB%8C%D9%86%DA%A9%DB%8C\n",
      "403 Forbidden\n",
      "Request forbidden by administrative rules.\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      "asriran dates added successfully :)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "mehrnews in progress...\n",
      "\n",
      "source is : mehrnews\n",
      "source is : mehrnews\n",
      "source is : mehrnews\n",
      "mehrnews dates added successfully :)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "khabaronline in progress...\n",
      "\n",
      "source is : khabaronline\n",
      "source is : khabaronline\n",
      "source is : khabaronline\n",
      "khabaronline dates added successfully :)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "yjcnews in progress...\n",
      "\n",
      "source is : yjcnews\n",
      "source is : yjcnews\n",
      "source is : yjcnews\n",
      "yjcnews dates added successfully :)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "tasnimnews in progress...\n",
      "\n",
      "source is : tasnimnews\n",
      "source is : tasnimnews\n",
      "source is : tasnimnews\n",
      "tasnimnews dates added successfully :)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "farsnews in progress...\n",
      "\n",
      "source is : farsnews\n",
      "soup.find is empty!\n",
      "URL= https://www.farsnews.ir/news/14011105000019/%D8%B1%D9%88%D8%A7%DB%8C%D8%AA-%D9%85%D8%AA%D8%AE%D8%B5%D8%B5-%D9%88%D8%A7%DA%A9%D8%B3%D9%86-%D8%A7%D8%B2-%D9%BE%D8%B4%D8%AA-%D9%BE%D8%B1%D8%AF%D9%87-%D8%AE%DB%8C%D8%A7%D9%86%D8%AA-%DA%A9%D9%85%D9%BE%D8%A7%D9%86%DB%8C-%D9%81%D8%A7%DB%8C%D8%B2%D8%B1-%D9%81%D9%86%D8%A7%D9%88%D8%B1%DB%8C-mRNA-%D8%A8%D8%B1%D8%A7%DB%8C\n",
      "\n",
      "502 Bad Gateway\n",
      "\n",
      "502 Bad Gateway\n",
      "nginx\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      "source is : farsnews\n",
      "source is : farsnews\n",
      "farsnews dates added successfully :)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "varzesh3 in progress...\n",
      "\n",
      "source is : varzesh3\n",
      "source is : varzesh3\n",
      "source is : varzesh3\n",
      "varzesh3 dates added successfully :)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "isna in progress...\n",
      "\n",
      "source is : isna\n",
      "source is : isna\n",
      "source is : isna\n",
      "isna dates added successfully :)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "entekhab in progress...\n",
      "\n",
      "source is : entekhab\n",
      "source is : entekhab\n",
      "source is : entekhab\n",
      "entekhab dates added successfully :)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "anapress in progress...\n",
      "\n",
      "source is : anapress\n",
      "source is : anapress\n",
      "soup.find is empty!\n",
      "URL= https://ana.press/fa/news/639600/%D8%A7%D8%B3%D8%AA%D9%81%D8%A7%D8%AF%D9%87-%D8%A7%D8%B2-%D8%B8%D8%B1%D9%81%DB%8C%D8%AA%E2%80%8C-%D9%87%D9%86%D8%B1%DB%8C-%D8%AF%D8%A7%D9%86%D8%B4%DA%AF%D8%A7%D9%87-%D8%A2%D8%B2%D8%A7%D8%AF-%D8%A7%D8%B3%D9%84%D8%A7%D9%85%DB%8C-%D8%A8%D8%B1%D8%A7%DB%8C-%D8%AA%D9%88%D8%B3%D8%B9%D9%87-%D9%85%D8%B9%D8%A7%D8%B1%D9%81-%D8%AF%DB%8C%D9%86%DB%8C\n",
      "403 Forbidden\n",
      "Request forbidden by administrative rules.\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      "source is : anapress\n",
      "soup.find is empty!\n",
      "URL= https://ana.press/fa/news/639600/%D8%A7%D8%B3%D8%AA%D9%81%D8%A7%D8%AF%D9%87-%D8%A7%D8%B2-%D8%B8%D8%B1%D9%81%DB%8C%D8%AA%E2%80%8C-%D9%87%D9%86%D8%B1%DB%8C-%D8%AF%D8%A7%D9%86%D8%B4%DA%AF%D8%A7%D9%87-%D8%A2%D8%B2%D8%A7%D8%AF-%D8%A7%D8%B3%D9%84%D8%A7%D9%85%DB%8C-%D8%A8%D8%B1%D8%A7%DB%8C-%D8%AA%D9%88%D8%B3%D8%B9%D9%87-%D9%85%D8%B9%D8%A7%D8%B1%D9%81-%D8%AF%DB%8C%D9%86%DB%8C\n",
      "403 Forbidden\n",
      "Request forbidden by administrative rules.\n",
      "\n",
      "\n",
      "--------------------\n",
      "\n",
      "anapress dates added successfully :)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "shana in progress...\n",
      "\n",
      "source is : shana\n",
      "source is : shana\n",
      "source is : shana\n",
      "shana dates added successfully :)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "jahannews in progress...\n",
      "\n",
      "source is : jahannews\n",
      "source is : jahannews\n",
      "source is : jahannews\n",
      "jahannews dates added successfully :)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "mashreghnews in progress...\n",
      "\n",
      "source is : mashreghnews\n",
      "source is : mashreghnews\n",
      "source is : mashreghnews\n",
      "mashreghnews dates added successfully :)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "iscanews in progress...\n",
      "\n",
      "source is : iscanews\n",
      "source is : iscanews\n",
      "source is : iscanews\n",
      "iscanews dates added successfully :)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Preproccesor called...\n",
      "Start dropping NaN values...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d764a85771e44ea0affbb1b66988031c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asriran in progress...\n",
      "\n",
      "Number of rows before dropping NaNs: 9\n",
      "Number of rows after dropping NaNs: 4\n",
      "asriran NaNs dropped successfully :)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "mehrnews in progress...\n",
      "\n",
      "Number of rows before dropping NaNs: 9\n",
      "Number of rows after dropping NaNs: 9\n",
      "mehrnews NaNs dropped successfully :)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "khabaronline in progress...\n",
      "\n",
      "Number of rows before dropping NaNs: 9\n",
      "Number of rows after dropping NaNs: 9\n",
      "khabaronline NaNs dropped successfully :)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "yjcnews in progress...\n",
      "\n",
      "Number of rows before dropping NaNs: 9\n",
      "Number of rows after dropping NaNs: 9\n",
      "yjcnews NaNs dropped successfully :)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "tasnimnews in progress...\n",
      "\n",
      "Number of rows before dropping NaNs: 9\n",
      "Number of rows after dropping NaNs: 9\n",
      "tasnimnews NaNs dropped successfully :)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "farsnews in progress...\n",
      "\n",
      "Number of rows before dropping NaNs: 9\n",
      "Number of rows after dropping NaNs: 8\n",
      "farsnews NaNs dropped successfully :)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "varzesh3 in progress...\n",
      "\n",
      "Number of rows before dropping NaNs: 9\n",
      "Number of rows after dropping NaNs: 9\n",
      "varzesh3 NaNs dropped successfully :)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "isna in progress...\n",
      "\n",
      "Number of rows before dropping NaNs: 9\n",
      "Number of rows after dropping NaNs: 9\n",
      "isna NaNs dropped successfully :)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "entekhab in progress...\n",
      "\n",
      "Number of rows before dropping NaNs: 9\n",
      "Number of rows after dropping NaNs: 9\n",
      "entekhab NaNs dropped successfully :)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "anapress in progress...\n",
      "\n",
      "Number of rows before dropping NaNs: 9\n",
      "Number of rows after dropping NaNs: 7\n",
      "anapress NaNs dropped successfully :)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "shana in progress...\n",
      "\n",
      "Number of rows before dropping NaNs: 9\n",
      "Number of rows after dropping NaNs: 9\n",
      "shana NaNs dropped successfully :)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "jahannews in progress...\n",
      "\n",
      "Number of rows before dropping NaNs: 9\n",
      "Number of rows after dropping NaNs: 9\n",
      "jahannews NaNs dropped successfully :)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "mashreghnews in progress...\n",
      "\n",
      "Number of rows before dropping NaNs: 9\n",
      "Number of rows after dropping NaNs: 9\n",
      "mashreghnews NaNs dropped successfully :)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "iscanews in progress...\n",
      "\n",
      "Number of rows before dropping NaNs: 9\n",
      "Number of rows after dropping NaNs: 9\n",
      "iscanews NaNs dropped successfully :)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Process of dropping NaNs completed successfully :)\n",
      "Start removing duplicate rows...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0195d467f4564a6d87387470ed37984f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asriran in progress...\n",
      "\n",
      "Number of rows before removing duplicates: 4\n",
      "Number of rows after removing duplicates: 3\n",
      "asriran Duplicate rows removed successfully :)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "mehrnews in progress...\n",
      "\n",
      "Number of rows before removing duplicates: 9\n",
      "Number of rows after removing duplicates: 6\n",
      "mehrnews Duplicate rows removed successfully :)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "khabaronline in progress...\n",
      "\n",
      "Number of rows before removing duplicates: 9\n",
      "Number of rows after removing duplicates: 7\n",
      "khabaronline Duplicate rows removed successfully :)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "yjcnews in progress...\n",
      "\n",
      "Number of rows before removing duplicates: 9\n",
      "Number of rows after removing duplicates: 7\n",
      "yjcnews Duplicate rows removed successfully :)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "tasnimnews in progress...\n",
      "\n",
      "Number of rows before removing duplicates: 9\n",
      "Number of rows after removing duplicates: 6\n",
      "tasnimnews Duplicate rows removed successfully :)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "farsnews in progress...\n",
      "\n",
      "Number of rows before removing duplicates: 8\n",
      "Number of rows after removing duplicates: 7\n",
      "farsnews Duplicate rows removed successfully :)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "varzesh3 in progress...\n",
      "\n",
      "Number of rows before removing duplicates: 9\n",
      "Number of rows after removing duplicates: 5\n",
      "varzesh3 Duplicate rows removed successfully :)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "isna in progress...\n",
      "\n",
      "Number of rows before removing duplicates: 9\n",
      "Number of rows after removing duplicates: 9\n",
      "isna Duplicate rows removed successfully :)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "entekhab in progress...\n",
      "\n",
      "Number of rows before removing duplicates: 9\n",
      "Number of rows after removing duplicates: 8\n",
      "entekhab Duplicate rows removed successfully :)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "anapress in progress...\n",
      "\n",
      "Number of rows before removing duplicates: 7\n",
      "Number of rows after removing duplicates: 4\n",
      "anapress Duplicate rows removed successfully :)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "shana in progress...\n",
      "\n",
      "Number of rows before removing duplicates: 9\n",
      "Number of rows after removing duplicates: 7\n",
      "shana Duplicate rows removed successfully :)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "jahannews in progress...\n",
      "\n",
      "Number of rows before removing duplicates: 9\n",
      "Number of rows after removing duplicates: 8\n",
      "jahannews Duplicate rows removed successfully :)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "mashreghnews in progress...\n",
      "\n",
      "Number of rows before removing duplicates: 9\n",
      "Number of rows after removing duplicates: 6\n",
      "mashreghnews Duplicate rows removed successfully :)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "iscanews in progress...\n",
      "\n",
      "Number of rows before removing duplicates: 9\n",
      "Number of rows after removing duplicates: 8\n",
      "iscanews Duplicate rows removed successfully :)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Process of removing duplicates completed successfully :)\n",
      "Start appending dataframes...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a01336541c6b44968a4f7bb017b97293",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asriran in progress...\n",
      "\n",
      "asriran appended successfully :)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "mehrnews in progress...\n",
      "\n",
      "mehrnews appended successfully :)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "khabaronline in progress...\n",
      "\n",
      "khabaronline appended successfully :)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "yjcnews in progress...\n",
      "\n",
      "yjcnews appended successfully :)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "tasnimnews in progress...\n",
      "\n",
      "tasnimnews appended successfully :)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "farsnews in progress...\n",
      "\n",
      "farsnews appended successfully :)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "varzesh3 in progress...\n",
      "\n",
      "varzesh3 appended successfully :)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "isna in progress...\n",
      "\n",
      "isna appended successfully :)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "entekhab in progress...\n",
      "\n",
      "entekhab appended successfully :)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "anapress in progress...\n",
      "\n",
      "anapress appended successfully :)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "shana in progress...\n",
      "\n",
      "shana appended successfully :)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "jahannews in progress...\n",
      "\n",
      "jahannews appended successfully :)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "mashreghnews in progress...\n",
      "\n",
      "mashreghnews appended successfully :)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "iscanews in progress...\n",
      "\n",
      "iscanews appended successfully :)\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Process of appending completed successfully :)\n",
      "Start merging dataframes...\n",
      "Process of merging completed successfully :)\n",
      "Start normalizing...\n",
      "Normalizing finished\n",
      "Start tokenizing...\n",
      "tokenizing finished\n",
      "stopwords removed!\n",
      "Postproccessor is called...\n",
      "punctuations removed!\n",
      "single words removed!\n",
      "trend extractor is called...\n",
      "The overall trend is خبرگزاری with 19 occurence\n",
      "\n",
      "The top 10 trends are [('خبرگزاری', 19), ('اخبار', 16), ('جهان', 14), ('نیوز', 14), ('تسنیم', 12), ('Tasnim', 12), ('فیلم', 10), ('نفت', 9), ('ایسنا', 9), ('ایران', 8)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    path = '/home/afsharino/Desktop/Trend-extraction-from-news/datasets/*csv'\n",
    "    # Read Data\n",
    "    data_reader = DataReader()\n",
    "    data_reader.read_dataset(path)\n",
    "    data_reader.add_all_titles()\n",
    "    data_reader.add_all_dates()\n",
    "    \n",
    "    # Preproccess\n",
    "    d = data_reader.data.copy()\n",
    "    \n",
    "    preproccessor = Preproccessor(d)\n",
    "    preproccessor.drop_nan_values()\n",
    "    preproccessor.remove_duplicate_rows()\n",
    "    preproccessor.merge_data_frames()\n",
    "    preproccessor.normalize()\n",
    "    preproccessor.tokenize()\n",
    "    preproccessor.remove_stopwords()\n",
    "    \n",
    "    # Postproccess\n",
    "    d = preproccessor.data\n",
    "    postproccessor = Postproccessor(d)\n",
    "    postproccessor.remove_punctuations()\n",
    "    postproccessor.remove_single_chars()\n",
    "    \n",
    "    # Trend Extraction\n",
    "    d = postproccessor.data\n",
    "    trend_extractor = TrenExtractor(d)\n",
    "    trend_extractor.find_overall_trend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea5aa49",
   "metadata": {},
   "source": [
    "Note: Due to a lack of time I just select 20 rows from each dataset to just show you the output but the code works properly until 772 rows and after that there are some lines you should skip because of problem in csv files."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
